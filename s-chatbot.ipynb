{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":[],"name":"sequential_chatbot_workflow.ipynb"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12089507,"sourceType":"datasetVersion","datasetId":7610460}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nourish/s-chatbot?scriptVersionId=244448176\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"id":"fd081859-932b-4780-bce3-d3c1fa2d4029","cell_type":"markdown","source":"\n**Model Choice: DistilGPT2**\nWe use `distilgpt2` for this task because it offers a good balance of performance, smaller size, and faster training/inference times compared to larger models. It's a practical choice for a commercially usable chatbot without requiring immense computational resources.\n","metadata":{}},{"id":"8978d6d9-c690-4b95-808d-93c024dc8681","cell_type":"code","source":"# Cell 1: Imports and Global Setup\n# This cell imports all necessary libraries and sets up global configurations like random seeds and device (CPU/GPU).\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import default_data_collator as DataCollatorForCausalLM\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments, \n    Trainer,\n    pipeline\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport json\nimport os\nfrom datetime import datetime\nimport re\nfrom collections import Counter\nimport string\nimport warnings\nwarnings.filterwarnings('ignore') # Ignore warnings for cleaner output\n\n# --- Global Configurations ---\n# Set random seeds for reproducibility across runs.\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\nmp.set_start_method('spawn', force=True)\n\n# Determine the device to use (GPU if available, otherwise CPU).\n# Kaggle P100 GPU is automatically detected if available.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define global constants for paths and model names.\nCSV_FILE = \"/kaggle/input/ptoject/combined_dataset.csv\" # Path to your dataset on Kaggle.\nMODEL_NAME = \"distilgpt2\"  # The pre-trained model to fine-tune.\nOUTPUT_DIR = \"./chatbot_model\"  # Directory to save the fine-tuned model and logs.\n\n# Training parameters\nNUM_EPOCHS = 5   # Number of full passes through the training dataset.\nBATCH_SIZE = 32   # Number of samples processed in one forward/backward pass.\nLEARNING_RATE = 2e-5 # Initial learning rate for the optimizer.\nMAX_LENGTH = 512 # Maximum sequence length for tokenization.\n\nprint(\"Global settings initialized:\")\nprint(f\"  Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    print(\"  GPU not available. Training and inference will use CPU (slower).\")\nprint(f\"  CSV File Path: {CSV_FILE}\")\nprint(f\"  Base Model: {MODEL_NAME}\")\nprint(f\"  Output Directory: {OUTPUT_DIR}\")\nprint(\"All necessary libraries imported and configurations set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:45:48.208984Z","iopub.execute_input":"2025-06-09T02:45:48.209665Z","iopub.status.idle":"2025-06-09T02:45:48.221062Z","shell.execute_reply.started":"2025-06-09T02:45:48.209631Z","shell.execute_reply":"2025-06-09T02:45:48.220384Z"}},"outputs":[{"name":"stdout","text":"Global settings initialized:\n  Device: cuda\n  GPU Name: Tesla P100-PCIE-16GB\n  GPU Memory: 15.9 GB\n  CSV File Path: /kaggle/input/ptoject/combined_dataset.csv\n  Base Model: distilgpt2\n  Output Directory: ./chatbot_model\nAll necessary libraries imported and configurations set.\n","output_type":"stream"}],"execution_count":35},{"id":"598ca950-e9c2-4dca-a69a-07c6e07d6c17","cell_type":"code","source":"# Cell 2: Data Loading and Preprocessing\n# This cell loads your conversational data from the specified CSV file and prepares it for model training.\n\nprint(\"PHASE 1: Data Loading and Preprocessing\")\nprint(\"=\" * 40)\n\nMODEL_NAME = \"gpt2\"  # or your chosen model name\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(f\"Tokenizer for model '{MODEL_NAME}' loaded successfully.\")\n\ntry:\n    def clean_text(text):\n        text = str(text)\n        text = text.strip()\n        text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces/newlines\n        return text\n        \n    df = pd.read_csv(CSV_FILE)\n    print(f\"Successfully loaded CSV from: {CSV_FILE}\")\n    print(f\"Initial DataFrame shape: {df.shape}\")\n    \n    if 'question' not in df.columns or 'support' not in df.columns:\n        raise ValueError(\"CSV must contain 'question' and 'support' columns. Please check your dataset.\")\n    \n    df = df.dropna(subset=['question', 'support']) # Remove rows with missing Q/A\n    print(f\"DataFrame shape after dropping NaNs: {df.shape}\")\n\n    # Format each question-answer pair into a single string.\n    # The <|endoftext|> token acts as a conversation turn separator for models like DistilGPT2.\n    formatted_texts = []\n    token_lens = []\n    for _, row in df.iterrows():\n        question = clean_text(row['question'])\n        support = clean_text(row['support'])\n        formatted_text = f\"Question: {question} Answer: {support}<|endoftext|>\"\n\n        tokens = tokenizer.encode(formatted_text)\n        token_lens.append(len(tokens))\n    \n        formatted_texts.append((formatted_text, len(tokens)))\n\n    \n    print(f\"Formatted {len(formatted_texts)} question-answer pairs for training.\")\n\n    min_len = 32\n    max_len = int(MAX_LENGTH * 0.9)\n\n    filtered_texts = [\n        text for text, l in formatted_texts\n        if min_len <= l <= max_len\n    ]\n\n    print(f\"Kept {len(filtered_texts)} / {len(formatted_texts)} pairs after length filtering.\")\n    \n    # Split the formatted data into training and validation sets.\n    train_texts, val_texts = train_test_split(\n        filtered_texts,\n        test_size=0.2,\n        random_state=42\n    )\n    \n    print(f\"Data split: {len(train_texts)} training samples, {len(val_texts)} validation samples.\")\n\nexcept FileNotFoundError:\n    print(f\"ERROR: The CSV file '{CSV_FILE}' was not found. Please ensure it's in the correct Kaggle input path.\")\nexcept Exception as e:\n    print(f\"An error occurred during data loading/preprocessing: {e}\")\n\nprint(\"Data loading and preprocessing complete.\")\n\nnumeric_df = df.select_dtypes(include=[np.number])\nprint(f\"Contains NaN: {numeric_df.isna().any().any()} | Contains Inf: {np.isinf(numeric_df.values).any()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:45:48.222214Z","iopub.execute_input":"2025-06-09T02:45:48.222412Z","iopub.status.idle":"2025-06-09T02:45:54.866754Z","shell.execute_reply.started":"2025-06-09T02:45:48.222397Z","shell.execute_reply":"2025-06-09T02:45:54.866006Z"}},"outputs":[{"name":"stdout","text":"PHASE 1: Data Loading and Preprocessing\n========================================\nTokenizer for model 'gpt2' loaded successfully.\nSuccessfully loaded CSV from: /kaggle/input/ptoject/combined_dataset.csv\nInitial DataFrame shape: (13679, 2)\nDataFrame shape after dropping NaNs: (12252, 2)\nFormatted 12252 question-answer pairs for training.\nKept 11496 / 12252 pairs after length filtering.\nData split: 9196 training samples, 2300 validation samples.\nData loading and preprocessing complete.\nContains NaN: False | Contains Inf: False\n","output_type":"stream"}],"execution_count":36},{"id":"283b13bb-5146-4407-8b99-eee673fb8166","cell_type":"code","source":"print(\"PHASE 2: Tokenizer Setup\")\nprint(\"=\" * 40)\n\ntry:\n    # Add a padding token if the tokenizer doesn't have one\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        print(f\"Tokenizer's pad token set to EOS token (ID: {tokenizer.pad_token_id}).\")\n    else:\n        print(f\"Tokenizer already has a pad token (ID: {tokenizer.pad_token_id}).\")\n    \n    # Force right padding\n    tokenizer.padding_side = \"right\"\n    print(f\"Tokenizer padding_side set to: {tokenizer.padding_side}\")\n    \n    # Print special tokens\n    print(f\"Special tokens:\")\n    print(f\"  pad_token: '{tokenizer.pad_token}' -> {tokenizer.pad_token_id}\")\n    print(f\"  eos_token: '{tokenizer.eos_token}' -> {tokenizer.eos_token_id}\")\n    print(f\"  bos_token: '{tokenizer.bos_token}' -> {tokenizer.bos_token_id if tokenizer.bos_token else 'None'}\")\n    print(f\"  unk_token: '{tokenizer.unk_token}' -> {tokenizer.unk_token_id}\")\n\n    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n\nexcept Exception as e:\n    print(f\"An error occurred during tokenizer setup: {e}\")\n\nprint(\"Tokenizer setup complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:45:54.867601Z","iopub.execute_input":"2025-06-09T02:45:54.86789Z","iopub.status.idle":"2025-06-09T02:45:54.882597Z","shell.execute_reply.started":"2025-06-09T02:45:54.867866Z","shell.execute_reply":"2025-06-09T02:45:54.881832Z"}},"outputs":[{"name":"stdout","text":"PHASE 2: Tokenizer Setup\n========================================\nTokenizer's pad token set to EOS token (ID: 50256).\nTokenizer padding_side set to: right\nSpecial tokens:\n  pad_token: '<|endoftext|>' -> 50256\n  eos_token: '<|endoftext|>' -> 50256\n  bos_token: '<|endoftext|>' -> 50256\n  unk_token: '<|endoftext|>' -> 50256\nTokenizer vocabulary size: 50257\nTokenizer setup complete.\n","output_type":"stream"}],"execution_count":37},{"id":"22669189-466d-4363-a896-0d942f64eb96","cell_type":"code","source":"class ChatbotDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': self.labels[idx]\n        }\n\n# Function to create dataset instance (no longer defines the class internally)\ndef create_chatbot_dataset(encodings, labels):\n    return ChatbotDataset(encodings, labels)\n\n\n# --- PHASE 3: Dataset Creation ---\nprint(\"PHASE 3: Dataset Creation\")\nprint(\"=\" * 40)\n\ntry:\n    # Tokenize training texts\n    train_encodings = tokenizer(\n        train_texts,\n        truncation=True,\n        padding='max_length',\n        max_length=MAX_LENGTH,\n        return_tensors='pt'\n    )\n\n    # Tokenize validation texts\n    val_encodings = tokenizer(\n        val_texts,\n        truncation=True,\n        padding='max_length',\n        max_length=MAX_LENGTH,\n        return_tensors='pt'\n    )\n\n    # Prepare labels\n    # def prepare_labels(encodings):\n    #     labels = encodings['input_ids'].clone()\n    #     # Set attention_mask == 0 to -100 for ignore_index in loss calculation\n    #     labels[encodings['attention_mask'] == 0] = -100\n    #     return labels\n\n    # train_labels = prepare_labels(train_encodings)\n    # val_labels = prepare_labels(val_encodings)\n\n    train_labels = train_encodings['input_ids'].clone() # No -100 assignment here\n    val_labels = val_encodings['input_ids'].clone()   # No -100 assignment here\n\n    # Create datasets using your new function\n    train_dataset = create_chatbot_dataset(train_encodings, train_labels)\n    val_dataset = create_chatbot_dataset(val_encodings, val_labels)\n\n    print(f\"Training Dataset created with {len(train_dataset)} samples.\")\n    print(f\"Validation Dataset created with {len(val_dataset)} samples.\")\n\n    # Optional check\n    sample = train_dataset[0]\n    print(\"\\nSample from training dataset (first entry):\")\n    print(f\"  Input IDs shape: {sample['input_ids'].shape}\")\n    print(f\"  Attention Mask shape: {sample['attention_mask'].shape}\")\n    print(f\"  Labels shape: {sample['labels'].shape}\")\n    print(f\"  Decoded Input: {tokenizer.decode(sample['input_ids'], skip_special_tokens=True)[:100]}...\")\n\nexcept Exception as e:\n    print(f\"An error occurred during dataset creation: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"Dataset creation complete.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:45:54.884262Z","iopub.execute_input":"2025-06-09T02:45:54.884497Z","iopub.status.idle":"2025-06-09T02:45:58.608709Z","shell.execute_reply.started":"2025-06-09T02:45:54.884473Z","shell.execute_reply":"2025-06-09T02:45:58.608001Z"}},"outputs":[{"name":"stdout","text":"PHASE 3: Dataset Creation\n========================================\nTraining Dataset created with 9196 samples.\nValidation Dataset created with 2300 samples.\n\nSample from training dataset (first entry):\n  Input IDs shape: torch.Size([512])\n  Attention Mask shape: torch.Size([512])\n  Labels shape: torch.Size([512])\n  Decoded Input: Question: What are made on ribosomes in the cytoplasm? Answer: DNA is located in the nucleus. Protei...\nDataset creation complete.\n","output_type":"stream"}],"execution_count":38},{"id":"88560a88-2ff0-49f6-bfed-bc2618e86446","cell_type":"code","source":"# Cell 5: Model Setup\n# This cell loads the pre-trained DistilGPT2 model and moves it to the appropriate device (GPU/CPU).\n\nprint(\"PHASE 4: Model Setup\")\nprint(\"=\" * 40)\n\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        # Use mixed precision (float16) if a GPU is available to save memory and speed up training.\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        # Automatically map model to available devices (e.g., multiple GPUs if configured).\n        device_map=\"auto\" if torch.cuda.is_available() else None\n    ).to(device)  # Ensure the model is explicitly moved to the selected device.\n    \n    print(f\"Model '{MODEL_NAME}' loaded successfully and moved to {device}.\")\n    print(f\"Model total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    # --- Safety: Check that tokenizer + model embeddings are aligned ---\n    vocab_size_tokenizer = len(tokenizer)\n    vocab_size_model = model.get_input_embeddings().num_embeddings\n    \n    print(f\"Tokenizer vocab size: {vocab_size_tokenizer}\")\n    print(f\"Model embedding size: {vocab_size_model}\")\n    \n    if vocab_size_tokenizer != vocab_size_model:\n        print(\"Mismatch detected! Resizing model embeddings...\")\n        model.resize_token_embeddings(vocab_size_tokenizer)\n        print(\"Resized model embeddings.\")\n\n    # --- Safety: Check that pad token and endoftext token exist ---\n    if tokenizer.pad_token is None:\n        print(\"WARNING: Tokenizer pad_token is None! Setting pad_token to eos_token...\")\n        tokenizer.pad_token = tokenizer.eos_token\n        model.resize_token_embeddings(len(tokenizer))\n        print(f\"Pad token set to eos_token. New vocab size: {len(tokenizer)}\")\n\n    if \"<|endoftext|>\" not in tokenizer.get_vocab():\n        print(\"WARNING: <|endoftext|> token not found in tokenizer vocab!\")\n    else:\n        print(f\"<|endoftext|> token ID: {tokenizer.convert_tokens_to_ids('<|endoftext|>')}\")\n\n    # --- Debugging: Print any frozen parameters ---\n    print(\"\\nChecking if any model parameters are frozen...\")\n    frozen_count = 0\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            print(f\"  FROZEN PARAM: {name}\")\n            frozen_count += 1\n\n    if frozen_count == 0:\n        print(\"All model parameters are trainable.\")\n    else:\n        print(f\"{frozen_count} parameters are frozen! This may affect training.\")\n\nexcept Exception as e:\n    print(f\"An error occurred during model setup: {e}\")\n\nprint(\"Model setup complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:45:58.609415Z","iopub.execute_input":"2025-06-09T02:45:58.609679Z","iopub.status.idle":"2025-06-09T02:45:59.368754Z","shell.execute_reply.started":"2025-06-09T02:45:58.609662Z","shell.execute_reply":"2025-06-09T02:45:59.368124Z"}},"outputs":[{"name":"stdout","text":"PHASE 4: Model Setup\n========================================\nModel 'gpt2' loaded successfully and moved to cuda.\nModel total parameters: 124,439,808\nTokenizer vocab size: 50257\nModel embedding size: 50257\n<|endoftext|> token ID: 50256\n\nChecking if any model parameters are frozen...\nAll model parameters are trainable.\nModel setup complete.\n","output_type":"stream"}],"execution_count":39},{"id":"a5dbaf74-c924-4a9f-ae74-8f26ed5631e2","cell_type":"code","source":"print(\"PHASE 5: Training Arguments Configuration (FP16 = False)\")\nprint(\"=\" * 40)\n\nimport os\n# Create output dirs\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(os.path.join(OUTPUT_DIR, \"logs\"), exist_ok=True)\n\n# Define gradient accumulation steps\nGRADIENT_ACCUMULATION_STEPS = 4\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    overwrite_output_dir=True,\n    do_train=True,\n    do_eval=True,\n    do_predict=False,  # We are training + validating, not predicting here\n    eval_strategy=\"epoch\",  # Do eval at end of each epoch\n    save_strategy=\"epoch\",        # Save at end of each epoch\n    save_total_limit=2,           # Keep only last 2 checkpoints\n    save_safetensors=True,        # Use safetensors format\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    max_grad_norm=1.0,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    warmup_steps=100,\n    logging_dir=f'{OUTPUT_DIR}/logs',\n    logging_strategy=\"steps\",      # Log every N steps\n    logging_steps=10,              # Log every 10 steps\n    logging_nan_inf_filter=True,   # Helps debug NaN/Inf issues\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=False,                    # We are turning FP16 off explicitly\n    dataloader_pin_memory=True,\n    remove_unused_columns=False,   # Important for GPT-style models\n    gradient_checkpointing=True,   # Saves memory for large models\n    run_name=f\"{MODEL_NAME}_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n    disable_tqdm=False,\n    report_to=\"none\",              # No W&B or other tracking\n    seed=42,\n    max_steps=-1,                  # No limit on steps (train fully)\n    save_steps=None,               # Not used, as we save per epoch\n    eval_steps=None,               # Not used, as we evaluate per epoch\n    logging_first_step=True,       # Log first step for debug clarity\n    dataloader_num_workers=0,      # You can increase this if needed (2 is safe default)\n    gradient_checkpointing_kwargs=None, # Default\n    optim=\"adamw_torch_fused\",     # Use PyTorch fused AdamW (faster if supported)\n)\n\n# Print args\nprint(\"Training arguments configured (FP16 = False):\")\nfor arg, value in training_args.to_dict().items():\n    if arg not in [\"__post_init__\", \"_setup_devices\"]:\n        print(f\"  {arg}: {value}\")\nprint(f\"  Effective Training Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n\nprint(\"Training arguments setup complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:45:59.369488Z","iopub.execute_input":"2025-06-09T02:45:59.369759Z","iopub.status.idle":"2025-06-09T02:45:59.402256Z","shell.execute_reply.started":"2025-06-09T02:45:59.369735Z","shell.execute_reply":"2025-06-09T02:45:59.401691Z"}},"outputs":[{"name":"stdout","text":"PHASE 5: Training Arguments Configuration (FP16 = False)\n========================================\nTraining arguments configured (FP16 = False):\n  output_dir: ./chatbot_model\n  overwrite_output_dir: True\n  do_train: True\n  do_eval: True\n  do_predict: False\n  eval_strategy: epoch\n  prediction_loss_only: False\n  per_device_train_batch_size: 32\n  per_device_eval_batch_size: 32\n  per_gpu_train_batch_size: None\n  per_gpu_eval_batch_size: None\n  gradient_accumulation_steps: 4\n  eval_accumulation_steps: None\n  eval_delay: 0\n  torch_empty_cache_steps: None\n  learning_rate: 2e-05\n  weight_decay: 0.01\n  adam_beta1: 0.9\n  adam_beta2: 0.999\n  adam_epsilon: 1e-08\n  max_grad_norm: 1.0\n  num_train_epochs: 5\n  max_steps: -1\n  lr_scheduler_type: linear\n  lr_scheduler_kwargs: {}\n  warmup_ratio: 0.0\n  warmup_steps: 100\n  log_level: passive\n  log_level_replica: warning\n  log_on_each_node: True\n  logging_dir: ./chatbot_model/logs\n  logging_strategy: steps\n  logging_first_step: True\n  logging_steps: 10\n  logging_nan_inf_filter: True\n  save_strategy: epoch\n  save_steps: None\n  save_total_limit: 2\n  save_safetensors: True\n  save_on_each_node: False\n  save_only_model: False\n  restore_callback_states_from_checkpoint: False\n  no_cuda: False\n  use_cpu: False\n  use_mps_device: False\n  seed: 42\n  data_seed: None\n  jit_mode_eval: False\n  use_ipex: False\n  bf16: False\n  fp16: False\n  fp16_opt_level: O1\n  half_precision_backend: auto\n  bf16_full_eval: False\n  fp16_full_eval: False\n  tf32: None\n  local_rank: 0\n  ddp_backend: None\n  tpu_num_cores: None\n  tpu_metrics_debug: False\n  debug: []\n  dataloader_drop_last: False\n  eval_steps: None\n  dataloader_num_workers: 0\n  dataloader_prefetch_factor: None\n  past_index: -1\n  run_name: gpt2_run_20250609_024559\n  disable_tqdm: False\n  remove_unused_columns: False\n  label_names: None\n  load_best_model_at_end: True\n  metric_for_best_model: eval_loss\n  greater_is_better: False\n  ignore_data_skip: False\n  fsdp: []\n  fsdp_min_num_params: 0\n  fsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\n  tp_size: 0\n  fsdp_transformer_layer_cls_to_wrap: None\n  accelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\n  deepspeed: None\n  label_smoothing_factor: 0.0\n  optim: adamw_torch_fused\n  optim_args: None\n  adafactor: False\n  group_by_length: False\n  length_column_name: length\n  report_to: []\n  ddp_find_unused_parameters: None\n  ddp_bucket_cap_mb: None\n  ddp_broadcast_buffers: None\n  dataloader_pin_memory: True\n  dataloader_persistent_workers: False\n  skip_memory_metrics: True\n  use_legacy_prediction_loop: False\n  push_to_hub: False\n  resume_from_checkpoint: None\n  hub_model_id: None\n  hub_strategy: every_save\n  hub_token: <HUB_TOKEN>\n  hub_private_repo: None\n  hub_always_push: False\n  gradient_checkpointing: True\n  gradient_checkpointing_kwargs: None\n  include_inputs_for_metrics: False\n  include_for_metrics: []\n  eval_do_concat_batches: True\n  fp16_backend: auto\n  push_to_hub_model_id: None\n  push_to_hub_organization: None\n  push_to_hub_token: <PUSH_TO_HUB_TOKEN>\n  mp_parameters: \n  auto_find_batch_size: False\n  full_determinism: False\n  torchdynamo: None\n  ray_scope: last\n  ddp_timeout: 1800\n  torch_compile: False\n  torch_compile_backend: None\n  torch_compile_mode: None\n  include_tokens_per_second: False\n  include_num_input_tokens_seen: False\n  neftune_noise_alpha: None\n  optim_target_modules: None\n  batch_eval_metrics: False\n  eval_on_start: False\n  use_liger_kernel: False\n  eval_use_gather_object: False\n  average_tokens_across_devices: False\n  Effective Training Batch Size: 128\nTraining arguments setup complete.\n","output_type":"stream"}],"execution_count":40},{"id":"4ea7ab59-aef0-492e-8468-6012dc15e6e5","cell_type":"code","source":"# Cell 7: Data Collator Setup\n# This cell sets up the data collator, which batches and dynamically pads sequences during training.\n\nprint(\"PHASE 6: Data Collator Setup\")\nprint(\"=\" * 40)\n\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n# Initialize the data collator for causal language modeling (CLM).\n# Setting mlm=False means we are doing next-token prediction, not masked language modeling.\n# data_collator = DataCollatorForCausalLM(tokenizer=tokenizer) \n\nprint(f\"Data collator for causal language modeling initialized using tokenizer '{MODEL_NAME}'.\")\nprint(\"Data collator setup complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:45:59.402991Z","iopub.execute_input":"2025-06-09T02:45:59.403275Z","iopub.status.idle":"2025-06-09T02:45:59.4078Z","shell.execute_reply.started":"2025-06-09T02:45:59.403258Z","shell.execute_reply":"2025-06-09T02:45:59.407062Z"}},"outputs":[{"name":"stdout","text":"PHASE 6: Data Collator Setup\n========================================\nData collator for causal language modeling initialized using tokenizer 'gpt2'.\nData collator setup complete.\n","output_type":"stream"}],"execution_count":41},{"id":"84d27c7f-31a2-41ca-9a98-c3fd85a7b932","cell_type":"code","source":"print(\"PHASE 7: Model Training\")\nprint(\"=\" * 40)\n\n# IMPORTANT: Wrap the training part inside if __name__ == '__main__':\n# This ensures that when multiprocessing spawns new processes, this code\n# is not re-executed, which can lead to issues with how modules are loaded.\n\nimport transformers\nif __name__ == '__main__':\n    try:\n        print(\"Initializing Hugging Face Trainer...\")\n        transformers.logging.set_verbosity_error()  # Suppress unnecessary logging\n\n        # Patch model config to avoid warnings if needed\n        if not hasattr(model.config, \"loss_type\") or model.config.loss_type is None:\n            model.config.loss_type = \"ForCausalLMLoss\"\n            print(\"Patched model.config.loss_type to 'ForCausalLMLoss' to avoid warning.\")\n\n        # Initialize Trainer\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n        )\n\n        print(\"Hugging Face Trainer initialized. Starting training process...\")\n\n        # Train the model\n        train_result = trainer.train()\n\n        # Save model & tokenizer\n        final_model_save_path = os.path.join(OUTPUT_DIR, \"prototype_model\")\n        print(f\"Saving final model to {final_model_save_path}...\")\n        trainer.save_model(output_dir=final_model_save_path)\n        tokenizer.save_pretrained(save_directory=final_model_save_path)\n\n        print(f\"Training completed! Final model and tokenizer saved to: {final_model_save_path}\")\n\n        # Display training metrics\n        metrics = train_result.metrics\n        print(\"\\nTraining Metrics:\")\n        for key, value in metrics.items():\n            if isinstance(value, (float, int)):\n                print(f\"  {key}: {value:.4f}\")\n            else:\n                print(f\"  {key}: {value}\")\n\n        # Save training metadata\n        training_info = {\n            \"model_name\": MODEL_NAME,\n            \"num_epochs\": NUM_EPOCHS,\n            \"batch_size\": BATCH_SIZE,\n            \"learning_rate\": LEARNING_RATE,\n            \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"train_samples\": len(train_dataset),\n            \"val_samples\": len(val_dataset),\n            \"final_train_loss\": metrics.get('train_loss'),\n            \"final_eval_loss\": metrics.get('eval_loss')\n        }\n        with open(f\"{OUTPUT_DIR}/training_info.json\", \"w\") as f:\n            json.dump(training_info, f, indent=2)\n        print(f\"Training metadata saved to: {OUTPUT_DIR}/training_info.json\")\n\n    except Exception as e:\n        import traceback\n        print(f\"An error occurred during model training: {e}\")\n        traceback.print_exc()\n\nprint(\"Model training phase complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T02:45:59.408473Z","iopub.execute_input":"2025-06-09T02:45:59.408663Z","iopub.status.idle":"2025-06-09T03:53:46.695345Z","shell.execute_reply.started":"2025-06-09T02:45:59.408641Z","shell.execute_reply":"2025-06-09T03:53:46.694577Z"}},"outputs":[{"name":"stdout","text":"PHASE 7: Model Training\n========================================\nInitializing Hugging Face Trainer...\nPatched model.config.loss_type to 'ForCausalLMLoss' to avoid warning.\nHugging Face Trainer initialized. Starting training process...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [360/360 1:07:34, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving final model to ./chatbot_model/prototype_model...\nTraining completed! Final model and tokenizer saved to: ./chatbot_model/prototype_model\n\nTraining Metrics:\n  train_runtime: 4065.6954\n  train_samples_per_second: 11.3090\n  train_steps_per_second: 0.0890\n  total_flos: 12014207631360000.0000\n  train_loss: 80.7115\n  epoch: 5.0000\nTraining metadata saved to: ./chatbot_model/training_info.json\nModel training phase complete.\n","output_type":"stream"}],"execution_count":42},{"id":"6fc0629e-632d-48cd-94a7-dd66f3728393","cell_type":"code","source":"print(\"PHASE 8: Inference Model Loading\")\nprint(\"=\" * 40)\n\ntry:\n    inference_model_path = os.path.join(OUTPUT_DIR, \"prototype_model\")\n    print(f\"Attempting to load fine-tuned model from: {inference_model_path}\")\n    \n    # Load tokenizer used for the trained model\n    inference_tokenizer = AutoTokenizer.from_pretrained(inference_model_path)\n    \n    # Load the fine-tuned causal language model\n    inference_model = AutoModelForCausalLM.from_pretrained(\n        inference_model_path,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\" if torch.cuda.is_available() else None\n    )\n    \n    # Create a Hugging Face pipeline for easy text generation.\n    generator = pipeline(\n        \"text-generation\",\n        model=inference_model,\n        tokenizer=inference_tokenizer\n        # No need to specify device anymore!\n    )\n    \n    print(\"Fine-tuned model and tokenizer loaded successfully for inference.\")\n    print(f\"Inference pipeline initialized.\")\n\nexcept Exception as e:\n    print(f\"An error occurred during inference model loading: {e}\")\n    print(\"Please ensure the training phase completed successfully and model files exist.\")\n\nprint(\"Inference model loading complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:46.696232Z","iopub.execute_input":"2025-06-09T03:53:46.696737Z","iopub.status.idle":"2025-06-09T03:53:46.998239Z","shell.execute_reply.started":"2025-06-09T03:53:46.696709Z","shell.execute_reply":"2025-06-09T03:53:46.997491Z"}},"outputs":[{"name":"stdout","text":"PHASE 8: Inference Model Loading\n========================================\nAttempting to load fine-tuned model from: ./chatbot_model/prototype_model\nFine-tuned model and tokenizer loaded successfully for inference.\nInference pipeline initialized.\nInference model loading complete.\n","output_type":"stream"}],"execution_count":43},{"id":"a03fc044-b883-4296-a3db-b8ca90c37ced","cell_type":"code","source":"# Cell 10: KeywordMatcher Setup\n# This cell sets up the KeywordMatcher, which is used as a fallback or complementary mechanism to the LLM.\n\nprint(\"PHASE 9: Keyword Matcher Setup\")\nprint(\"=\" * 40)\n\n# Helper functions for KeywordMatcher\ndef preprocess_text(text):\n    \"\"\"Cleans and preprocesses a given text string for better matching.\"\"\"\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = ' '.join(text.split())\n    return text\n\ndef extract_keywords(text, top_n=10):\n    \"\"\"Extracts the most important keywords from a text based on frequency, after preprocessing.\"\"\"\n    processed_text = preprocess_text(text)\n    words = processed_text.split()\n    stop_words = {'what', 'how', 'when', 'where', 'why', 'who', 'which', 'is', 'are', 'was', 'were', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'do', 'does', 'did', 'can', 'could', 'should', 'would', 'will'}\n    keywords = [word for word in words if word not in stop_words and len(word) > 2]\n    keyword_counts = Counter(keywords)\n    return [word for word, count in keyword_counts.most_common(top_n)]\n\nclass KeywordMatcher:\n    \"\"\"\n    Class for keyword-based matching and similarity calculation using TF-IDF.\n    \"\"\"\n    def __init__(self):\n        self.vectorizer = TfidfVectorizer(\n            stop_words='english',\n            ngram_range=(1, 2),  \n            max_features=5000,\n            lowercase=True\n        )\n        self.question_vectors = None\n        self.original_questions = None\n        self.original_answers = None\n        \n    def fit(self, questions, answers):\n        \"\"\"Fits the TF-IDF vectorizer to a list of questions and stores original questions and answers.\"\"\"\n        self.original_questions = questions\n        self.original_answers = answers\n        processed_questions = [preprocess_text(q) for q in questions]\n        self.question_vectors = self.vectorizer.fit_transform(processed_questions)\n        print(f\"  Keyword matcher fitted on {len(questions)} questions.\")\n    \n    def find_best_match(self, user_question, similarity_threshold=0.3, keyword_threshold=0.4):\n        \"\"\"Finds the best matching question from the dataset based on TF-IDF cosine similarity and keyword overlap.\"\"\"\n        processed_user_question = preprocess_text(user_question)\n        user_vector = self.vectorizer.transform([processed_user_question])\n        similarities = cosine_similarity(user_vector, self.question_vectors).flatten()\n        top_indices = np.argsort(similarities)[::-1][:5] \n        user_keywords = set(extract_keywords(user_question))\n        \n        best_match_idx = None\n        best_score = 0\n        match_details = []\n        \n        for idx in top_indices:\n            similarity_score = similarities[idx]\n            dataset_keywords = set(extract_keywords(self.original_questions[idx]))\n            common_keywords = user_keywords.intersection(dataset_keywords)\n            keyword_score = len(common_keywords) / max(len(user_keywords), 1) if user_keywords else 0\n            combined_score = 0.6 * similarity_score + 0.4 * keyword_score\n            match_details.append({\n                'index': idx,\n                'question': self.original_questions[idx],\n                'answer': self.original_answers[idx],\n                'similarity_score': similarity_score,\n                'keyword_score': keyword_score,\n                'combined_score': combined_score,\n                'common_keywords': list(common_keywords)\n            })\n            \n            if combined_score > best_score and (similarity_score >= similarity_threshold or keyword_score >= keyword_threshold):\n                best_score = combined_score\n                best_match_idx = idx\n        match_details.sort(key=lambda x: x['combined_score'], reverse=True)\n        return best_match_idx, best_score, match_details\n\n# Initialize and fit the KeywordMatcher\nkeyword_matcher = KeywordMatcher()\nuse_fallback = True # Flag to enable/disable keyword-based fallback\n\ntry:\n    df_full = pd.read_csv(CSV_FILE) # Load full dataset for keyword matcher\n    questions_for_matcher = df_full['question'].fillna('').tolist()\n    answers_for_matcher = df_full['support'].fillna('').tolist()\n    keyword_matcher.fit(questions_for_matcher, answers_for_matcher)\n    print(\"Keyword matcher successfully set up and fitted with data.\")\n\nexcept Exception as e:\n    print(f\"WARNING: Could not set up keyword matcher: {e}. Keyword fallback will be disabled.\")\n    use_fallback = False\n\nprint(\"Keyword matcher setup complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:47.000185Z","iopub.execute_input":"2025-06-09T03:53:47.000646Z","iopub.status.idle":"2025-06-09T03:53:47.464716Z","shell.execute_reply.started":"2025-06-09T03:53:47.000626Z","shell.execute_reply":"2025-06-09T03:53:47.463942Z"}},"outputs":[{"name":"stdout","text":"PHASE 9: Keyword Matcher Setup\n========================================\n  Keyword matcher fitted on 13679 questions.\nKeyword matcher successfully set up and fitted with data.\nKeyword matcher setup complete.\n","output_type":"stream"}],"execution_count":44},{"id":"d9a4c411-f466-422d-bc99-ede41e6b7ba2","cell_type":"code","source":"# Cell 11: Inference Helper Functions\n# These helper functions assist the main `generate_response` logic by filtering out low-quality outputs.\n\nprint(\"PHASE 10: Defining Inference Helper Functions\")\nprint(\"=\" * 40)\n\ndef _is_generic_response(response: str) -> bool:\n    \"\"\"\n    Check if a generated response is too generic or indicates the model couldn't find a good answer.\n    Useful to decide when to trigger a fallback (like keyword matching).\n\n    Args:\n        response (str): The generated text from the model.\n\n    Returns:\n        bool: True if the response is generic/insufficient, False otherwise.\n    \"\"\"\n    generic_phrases = [\n        \"i'm sorry\", \"i don't know\", \"i can't help\", \"i'm not sure\",\n        \"i apologize\", \"sorry, i couldn't\", \"coherent response\", \"generate a proper response\"\n    ]\n    response_lower = response.lower()\n    \n    # Short responses are likely generic\n    if len(response.split()) < 5:\n        return True\n    \n    # Check if any generic phrase is present\n    return any(phrase in response_lower for phrase in generic_phrases)\n\ndef _is_all_padding(response: str) -> bool:\n    \"\"\"\n    Check if the generated response is mostly padding or empty after decoding.\n\n    Args:\n        response (str): The generated text from the model.\n\n    Returns:\n        bool: True if response is empty or too short (likely padding), False otherwise.\n    \"\"\"\n    return len(response.strip()) < 2\n\nprint(\"Inference helper functions `_is_generic_response` and `_is_all_padding` defined.\")\nprint(\"Inference helper functions setup complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:47.465484Z","iopub.execute_input":"2025-06-09T03:53:47.465735Z","iopub.status.idle":"2025-06-09T03:53:47.472057Z","shell.execute_reply.started":"2025-06-09T03:53:47.465716Z","shell.execute_reply":"2025-06-09T03:53:47.471347Z"}},"outputs":[{"name":"stdout","text":"PHASE 10: Defining Inference Helper Functions\n========================================\nInference helper functions `_is_generic_response` and `_is_all_padding` defined.\nInference helper functions setup complete.\n","output_type":"stream"}],"execution_count":45},{"id":"08b23b3d-0e00-44f1-9e2a-5bfb1f7b6f6f","cell_type":"code","source":"# Cell 12: Main Inference Function (`generate_response`)\n# This function combines LLM generation and keyword matching for a hybrid response strategy.\n\nprint(\"PHASE 11: Defining Main Inference Logic\")\nprint(\"=\" * 40)\n\ndef generate_response(question, max_length=150, temperature=0.2, top_p=0.9, use_keyword_fallback=True):\n    \"\"\"\n    Generates a response using a hybrid approach:\n    1. Tries keyword matching first for strong, direct answers.\n    2. Falls back to LLM generation if keyword match is moderate or weak,\n       and can use keyword match if LLM response is generic.\n    \"\"\"\n    global generator, keyword_matcher, use_fallback # Access global variables\n\n    # Step 1: Attempt keyword matching if enabled\n    if use_fallback and use_keyword_fallback:\n        match_idx, match_score, match_details = keyword_matcher.find_best_match(question)\n        \n        # If a strong keyword match is found, return its answer directly.\n        if match_idx is not None and match_score > 0.7:\n            print(f\"******************[Strategy: KEYWORD MATCH] Direct hit! Score: {match_score:.3f}*********************\")\n            return keyword_matcher.original_answers[match_idx]\n        \n        # If a moderate keyword match is found, try LLM first, but keep fallback as an option.\n        elif match_idx is not None and match_score > 0.4:\n            print(f\"******************[Strategy: HYBRID MODE] Moderate keyword match (score: {match_score:.3f}). Trying LLM first.*******************\")\n            \n            # Try to generate a response using the LLM\n            llm_response = _generate_llm_response_llm(question, max_length, temperature, top_p)\n            \n            # If the LLM's response is generic or unhelpful, use the keyword match as a fallback.\n            if _is_generic_response(llm_response) and not _is_all_padding(llm_response):\n                print(\"*****************[Fallback Action] LLM response was generic. Using keyword match.****************\")\n                return keyword_matcher.original_answers[match_idx]\n            \n            # Otherwise, if LLM generated a good response, use it.\n            return llm_response\n    \n    # Step 2: If no strong keyword match or fallback is disabled, default to LLM generation.\n    print(\"########################[Strategy: LLM MODE] No strong keyword match or fallback disabled. Generating with LLM.#################\")\n    return _generate_llm_response_llm(question, max_length, temperature, top_p)\n\ndef _generate_llm_response_llm(question, max_length, temperature, top_p):\n    \"\"\"\n    Generates a response using the fine-tuned Large Language Model (LLM) via the Hugging Face pipeline.\n    \"\"\"\n    global generator, inference_tokenizer # Access global pipeline and tokenizer\n    input_text = f\"Question: {question} Answer:\"\n    \n    response = generator(\n        input_text,\n        max_length=max_length,\n        num_beams=5,\n        do_sample=False,\n        no_repeat_ngram_size=2,\n        pad_token_id=inference_tokenizer.eos_token_id,\n        num_return_sequences=1\n    )\n    \n    generated_text = response[0]['generated_text']\n    \n    if \"Answer:\" in generated_text:\n        answer = generated_text.split(\"Answer:\")[-1].strip()\n        answer = answer.replace(\"<|endoftext|>\", \"\").strip()\n        return answer\n    \n    return \"I'm sorry, I couldn't generate a proper response from the LLM.\"\n\nprint(\"Main inference function `generate_response` and LLM specific generation `_generate_llm_response_llm` defined.\")\n\nprint(\"Inference logic setup complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:47.472846Z","iopub.execute_input":"2025-06-09T03:53:47.473083Z","iopub.status.idle":"2025-06-09T03:53:47.496847Z","shell.execute_reply.started":"2025-06-09T03:53:47.473059Z","shell.execute_reply":"2025-06-09T03:53:47.496184Z"}},"outputs":[{"name":"stdout","text":"PHASE 11: Defining Main Inference Logic\n========================================\nMain inference function `generate_response` and LLM specific generation `_generate_llm_response_llm` defined.\nInference logic setup complete.\n","output_type":"stream"}],"execution_count":46},{"id":"d2cbf2e0-2db1-491b-9979-e97fe4ba03de","cell_type":"code","source":"# Cell 13: Debugging Function (`get_match_details`)\n# This function provides detailed insights into how the keyword matcher found its matches.\n\nprint(\"PHASE 12: Defining Debugging Function\")\nprint(\"=\" * 40)\n\ndef get_match_details(question):\n    \"\"\"\n    Provides detailed information about the keyword matching process for a given question.\n    Useful for debugging and understanding why a certain keyword match was or wasn't chosen.\n    \"\"\"\n    global keyword_matcher, use_fallback # Access global variables\n    if not use_fallback:\n        return \"Keyword matching not available or disabled.\"\n    \n    match_idx, match_score, match_details = keyword_matcher.find_best_match(question)\n    \n    result = f\"User Question: {question}\\n\"\n    result += f\"Best Match Score: {match_score:.3f}\\n\\n\"\n    result += \"Top 3 Matches (ordered by combined score):\\n\"\n    result += \"-\" * 50 + \"\\n\"\n    \n    for i, detail in enumerate(match_details[:3]):\n        result += f\"{i+1}. Question (from dataset): {detail['question']}\\n\"\n        result += f\"   Cosine Similarity Score: {detail['similarity_score']:.3f}\\n\"\n        result += f\"   Keyword Overlap Score: {detail['keyword_score']:.3f}\\n\"\n        result += f\"   Combined Score: {detail['combined_score']:.3f}\\n\"\n        result += f\"   Common Keywords with User Query: {', '.join(detail['common_keywords'])}\\n\"\n        result += f\"   Corresponding Answer: {detail['answer'][:100]}...\\n\\n\" # Truncate answer for brevity\n    \n    return result\n\nprint(\"Debugging function `get_match_details` defined.\")\n\nprint(\"Debugging function setup complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:47.497523Z","iopub.execute_input":"2025-06-09T03:53:47.497726Z","iopub.status.idle":"2025-06-09T03:53:47.515871Z","shell.execute_reply.started":"2025-06-09T03:53:47.497703Z","shell.execute_reply":"2025-06-09T03:53:47.515074Z"}},"outputs":[{"name":"stdout","text":"PHASE 12: Defining Debugging Function\n========================================\nDebugging function `get_match_details` defined.\nDebugging function setup complete.\n","output_type":"stream"}],"execution_count":47},{"id":"de734312-9bc1-494b-8136-982c540a2d9b","cell_type":"code","source":"# Cell 14: Interactive Chat Function (`chat_interactive`)\n# This cell defines an interactive command-line interface for testing the chatbot.\n\nprint(\"PHASE 13: Defining Interactive Chat Function\")\nprint(\"=\" * 40)\n\ndef chat_interactive():\n    \"\"\"\n    Provides an enhanced interactive command-line chat interface for testing the chatbot.\n    Supports 'quit', 'exit', 'help', and 'debug' commands.\n    \"\"\"\n    print(\"\\n=== Chatbot Interactive Mode ===\")\n    print(\"Type 'quit' or 'exit' to end the conversation.\")\n    print(\"Type 'debug' to see keyword matching details for the last question asked.\")\n    print(\"Type 'help' for available commands.\")\n    print(\"=\" * 45 + \"\\n\")\n    \n    last_question = \"\" # Stores the last question asked for 'debug' command\n    \n    while True:\n        user_input = input(\"You: \").strip()\n        \n        if user_input.lower() in ['quit', 'exit']:\n            print(\"Goodbye! Exiting chat mode.\")\n            break\n        \n        if user_input.lower() == 'help':\n            print(\"\\nAvailable commands:\")\n            print(\"- 'debug': Show detailed keyword matching information for your last question.\")\n            print(\"- 'quit' or 'exit': Terminate the interactive chat session.\")\n            print(\"- Any other text: Ask a question to the chatbot.\")\n            print(\"\\n\")\n            continue\n        \n        if user_input.lower() == 'debug':\n            if last_question:\n                print(\"\\n\" + \"=\"*50)\n                print(\"KEYWORD MATCHING DEBUG INFO\")\n                print(\"=\"*50)\n                print(get_match_details(last_question)) # Call global function\n                print(\"=\"*50 + \"\\n\")\n            else:\n                print(\"No previous question to debug yet. Ask a question first.\\n\")\n            continue\n        \n        if not user_input:\n            print(\"Please type something to ask the chatbot.\\n\")\n            continue\n        \n        last_question = user_input # Store the current question for potential debugging\n        print(\"Bot: \", end=\"\")\n        response = generate_response(user_input) # Call global function\n        print(response)\n        print()\n\nprint(\"Interactive chat function `chat_interactive` defined.\")\n\nprint(\"Interactive chat function setup complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:47.516818Z","iopub.execute_input":"2025-06-09T03:53:47.517075Z","iopub.status.idle":"2025-06-09T03:53:47.538387Z","shell.execute_reply.started":"2025-06-09T03:53:47.517052Z","shell.execute_reply":"2025-06-09T03:53:47.537763Z"}},"outputs":[{"name":"stdout","text":"PHASE 13: Defining Interactive Chat Function\n========================================\nInteractive chat function `chat_interactive` defined.\nInteractive chat function setup complete.\n","output_type":"stream"}],"execution_count":48},{"id":"04e23726-e9a3-42b7-99ae-31300071b806","cell_type":"code","source":"# Cell 15: Batch Testing Function (`batch_test`)\n# This cell defines a function to test the chatbot with a predefined list of questions.\n\nprint(\"PHASE 14: Defining Batch Testing Function\")\nprint(\"=\" * 40)\n\ndef batch_test(test_questions):\n    \"\"\"\n    Tests the chatbot with a list of predefined questions and prints results.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"BATCH TESTING RESULTS\")\n    print(\"=\"*60)\n    \n    for i, question in enumerate(test_questions):\n        print(f\"\\nTest {i+1} - Q: {question}\")\n        response = generate_response(question) # Call global generate_response\n        print(f\"A: {response}\")\n        # Optionally, show keyword match details for each batch test question\n        # Uncomment the line below if you want to see verbose details for each test\n        # print(get_match_details(question))\n        print(\"--- End of Test ---\")\n    print(\"All predefined test questions processed in batch.\")\n\nprint(\"Batch testing function `batch_test` defined.\")\n\nprint(\"Batch testing function setup complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:47.538982Z","iopub.execute_input":"2025-06-09T03:53:47.539155Z","iopub.status.idle":"2025-06-09T03:53:47.564279Z","shell.execute_reply.started":"2025-06-09T03:53:47.539141Z","shell.execute_reply":"2025-06-09T03:53:47.563618Z"}},"outputs":[{"name":"stdout","text":"PHASE 14: Defining Batch Testing Function\n========================================\nBatch testing function `batch_test` defined.\nBatch testing function setup complete.\n","output_type":"stream"}],"execution_count":49},{"id":"98318099-8f2b-4f5c-a514-1ef8dc89db67","cell_type":"code","source":"# Cell 16: Main Execution - Testing and Interactive Chat\n# This cell executes the testing and interactive chat functions to demonstrate the chatbot's capabilities.\n\nprint(\"PHASE 15: Executing Testing and Interactive Chat\")\nprint(\"=\" * 40)\n\nif 'generator' not in locals() or generator is None:\n    print(\"ERROR: Inference model (generator) not loaded. Please run previous cells.\")\nelse:\n    # Define test questions for batch testing\n    batch_test_questions = [\n    \"What is carbon emission?\",\n    \"How does AI work?\",\n    \"Tell me about deep learning.\",\n    \"What are neural networks?\",\n    \"Who invented the light bulb?\",  # Question potentially outside training data domain\n    \"Hello, how are you?\",\n\n    # Environment-related questions:\n    \"What is climate change?\",\n    \"How does deforestation impact the environment?\",\n    \"What are renewable energy sources?\",\n    \"Explain the greenhouse effect.\",\n    \"What is global warming?\",\n    \"What are the effects of plastic pollution?\",\n    \"How can individuals reduce their carbon footprint?\",\n    \"What is biodiversity and why is it important?\",\n    \"What are the main causes of air pollution?\",\n    \"How do electric vehicles help the environment?\"\n]\n    \n    batch_test(batch_test_questions)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Starting interactive chat session...\")\n    print(\"=\"*60)\n    chat_interactive()\n    print(\"Interactive chat session ended.\")\n\nprint(\"All phases of the chatbot workflow finished.\")\nprint(\"\\n--- Chatbot Pipeline Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:47.565078Z","iopub.execute_input":"2025-06-09T03:53:47.565308Z","iopub.status.idle":"2025-06-09T03:54:09.255411Z","shell.execute_reply.started":"2025-06-09T03:53:47.565292Z","shell.execute_reply":"2025-06-09T03:54:09.254726Z"}},"outputs":[{"name":"stdout","text":"PHASE 15: Executing Testing and Interactive Chat\n========================================\n\n============================================================\nBATCH TESTING RESULTS\n============================================================\n\nTest 1 - Q: What is carbon emission?\n******************[Strategy: HYBRID MODE] Moderate keyword match (score: 0.532). Trying LLM first.*******************\nA: '\n--- End of Test ---\n\nTest 2 - Q: How does AI work?\n******************[Strategy: KEYWORD MATCH] Direct hit! Score: 0.896*********************\nA: \n--- End of Test ---\n\nTest 3 - Q: Tell me about deep learning.\n########################[Strategy: LLM MODE] No strong keyword match or fallback disabled. Generating with LLM.#################\nA: '\n--- End of Test ---\n\nTest 4 - Q: What are neural networks?\n******************[Strategy: HYBRID MODE] Moderate keyword match (score: 0.459). Trying LLM first.*******************\nA: '\n--- End of Test ---\n\nTest 5 - Q: Who invented the light bulb?\n******************[Strategy: HYBRID MODE] Moderate keyword match (score: 0.610). Trying LLM first.*******************\nA: '\n--- End of Test ---\n\nTest 6 - Q: Hello, how are you?\n########################[Strategy: LLM MODE] No strong keyword match or fallback disabled. Generating with LLM.#################\nA: '\n--- End of Test ---\n\nTest 7 - Q: What is climate change?\n******************[Strategy: KEYWORD MATCH] Direct hit! Score: 0.733*********************\nA: \n--- End of Test ---\n\nTest 8 - Q: How does deforestation impact the environment?\n******************[Strategy: HYBRID MODE] Moderate keyword match (score: 0.589). Trying LLM first.*******************\nA: '\n--- End of Test ---\n\nTest 9 - Q: What are renewable energy sources?\n******************[Strategy: HYBRID MODE] Moderate keyword match (score: 0.675). Trying LLM first.*******************\nA: '\n--- End of Test ---\n\nTest 10 - Q: Explain the greenhouse effect.\n******************[Strategy: KEYWORD MATCH] Direct hit! Score: 0.722*********************\nA: Greenhouse gases in the atmosphere absorb heat. This is called the greenhouse effect and it makes the planet warmer. Human actions have increased the greenhouse effect.\n--- End of Test ---\n\nTest 11 - Q: What is global warming?\n******************[Strategy: KEYWORD MATCH] Direct hit! Score: 0.908*********************\nA: The atmosphere contains small amounts of carbon dioxide. Climatologists have found that humans are putting a lot of extra carbon dioxide into the atmosphere. This is mostly from burning fossil fuels. The extra carbon dioxide traps heat from the Sun. Trapped heat causes the atmosphere to heat up. We call this global warming ( Figure below ).\n--- End of Test ---\n\nTest 12 - Q: What are the effects of plastic pollution?\n******************[Strategy: HYBRID MODE] Moderate keyword match (score: 0.424). Trying LLM first.*******************\nA: '\n--- End of Test ---\n\nTest 13 - Q: How can individuals reduce their carbon footprint?\n########################[Strategy: LLM MODE] No strong keyword match or fallback disabled. Generating with LLM.#################\nA: '\n--- End of Test ---\n\nTest 14 - Q: What is biodiversity and why is it important?\n******************[Strategy: HYBRID MODE] Moderate keyword match (score: 0.577). Trying LLM first.*******************\nA: '\n--- End of Test ---\n\nTest 15 - Q: What are the main causes of air pollution?\n******************[Strategy: KEYWORD MATCH] Direct hit! Score: 0.739*********************\nA: The main cause of outdoor air pollution is the burning of fossil fuels. Outdoor air pollution causes human health problems, acid rain, and global climate change.\n--- End of Test ---\n\nTest 16 - Q: How do electric vehicles help the environment?\n########################[Strategy: LLM MODE] No strong keyword match or fallback disabled. Generating with LLM.#################\nA: '\n--- End of Test ---\nAll predefined test questions processed in batch.\n\n============================================================\nStarting interactive chat session...\n============================================================\n\n=== Chatbot Interactive Mode ===\nType 'quit' or 'exit' to end the conversation.\nType 'debug' to see keyword matching details for the last question asked.\nType 'help' for available commands.\n=============================================\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"Goodbye! Exiting chat mode.\nInteractive chat session ended.\nAll phases of the chatbot workflow finished.\n\n--- Chatbot Pipeline Finished ---\n","output_type":"stream"}],"execution_count":50},{"id":"23499761-3df8-484e-8513-2621a43fde59","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9652548a-efdf-409e-ae68-9768d5da6f8f","cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"18f41eb3-5a5b-49d4-9a9d-0fdf2fb20357","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}